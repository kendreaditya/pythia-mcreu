{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1DO0QEuVcFF2okut5Qp7v623iri8W_xWk","timestamp":1689216224242},{"file_id":"https://github.com/samwit/llm-tutorials/blob/main/YT_Open_Assistant_Pythia12B_8bit_in_Colab.ipynb","timestamp":1687090407308}],"machine_shape":"hm","gpuClass":"premium","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium","widgets":{"application/vnd.jupyter.widget-state+json":{"297ac47098b04a0092230a1326d63297":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe9f857642594ebabd4413774b659f96","IPY_MODEL_17e3b30a755f44dd9d7d1a3810a4e942","IPY_MODEL_5a32e20ffe8947cf9ed7e7317b648e0d"],"layout":"IPY_MODEL_96c320604e044def8f4f2f63dcdd5214"}},"fe9f857642594ebabd4413774b659f96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a82bab1ac195424f8f84a507382940fc","placeholder":"​","style":"IPY_MODEL_197096a60f64431897ddd5cbbd8b030c","value":"Downloading (…)okenizer_config.json: 100%"}},"17e3b30a755f44dd9d7d1a3810a4e942":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e24dc38edff4cc9905005a0d21db099","max":396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13270b4ee02e4d718a9c2bcdf37ce5fa","value":396}},"5a32e20ffe8947cf9ed7e7317b648e0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71bdc370d1204f959410dfd9c45d8be1","placeholder":"​","style":"IPY_MODEL_a3a2a5cf28ce48e284c5587d38c576e7","value":" 396/396 [00:00&lt;00:00, 22.6kB/s]"}},"96c320604e044def8f4f2f63dcdd5214":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a82bab1ac195424f8f84a507382940fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"197096a60f64431897ddd5cbbd8b030c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e24dc38edff4cc9905005a0d21db099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13270b4ee02e4d718a9c2bcdf37ce5fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71bdc370d1204f959410dfd9c45d8be1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3a2a5cf28ce48e284c5587d38c576e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c10587a9648943a48594c24ca40a2bcb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee7b199328aa4710ae84d2a6b3fe9c31","IPY_MODEL_b91f9d78419340a49621bb6d00497332","IPY_MODEL_51279de2dc6d4852b952e7e3b3d8c781"],"layout":"IPY_MODEL_9771d783d0184e0c98d43f92009ae7cb"}},"ee7b199328aa4710ae84d2a6b3fe9c31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfb29608f23e4eccacaeb075b5bd40fa","placeholder":"​","style":"IPY_MODEL_b6f9dbba1e1247d0b28285230128dfac","value":"Downloading (…)/main/tokenizer.json: 100%"}},"b91f9d78419340a49621bb6d00497332":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8f5a7dbce0e48ee991d8baab614af21","max":2113710,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d618f01389e2410192dfe92bdc3c0a7b","value":2113710}},"51279de2dc6d4852b952e7e3b3d8c781":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_321a063db60c42f4b0bee979692744ea","placeholder":"​","style":"IPY_MODEL_6f146d6079a044d7982a36a6c231e371","value":" 2.11M/2.11M [00:00&lt;00:00, 14.1MB/s]"}},"9771d783d0184e0c98d43f92009ae7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfb29608f23e4eccacaeb075b5bd40fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6f9dbba1e1247d0b28285230128dfac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8f5a7dbce0e48ee991d8baab614af21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d618f01389e2410192dfe92bdc3c0a7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"321a063db60c42f4b0bee979692744ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f146d6079a044d7982a36a6c231e371":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c791678c071441fba62c8d88ee8ea9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19273b95d75b42d28cc40809f9e2f01e","IPY_MODEL_51980de8a43e40cd876c38c1155430d4","IPY_MODEL_6afe3a5327b34ae6a3e59e2cd341d926"],"layout":"IPY_MODEL_c446fb1d03064e228f834c84d2c08d10"}},"19273b95d75b42d28cc40809f9e2f01e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_416f35be8e884416b08951435764c6a3","placeholder":"​","style":"IPY_MODEL_7d61a7d10dd3483f8cc263317c38de79","value":"Downloading (…)cial_tokens_map.json: 100%"}},"51980de8a43e40cd876c38c1155430d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_864498126fd248c697679a0eb5d5583f","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08ec4fe354764ffb88ba0aeb06784d79","value":99}},"6afe3a5327b34ae6a3e59e2cd341d926":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5cdc6f5c16a4e61ac45e81f318880c9","placeholder":"​","style":"IPY_MODEL_3ec30153737d48feb0ad48840bd9a211","value":" 99.0/99.0 [00:00&lt;00:00, 1.88kB/s]"}},"c446fb1d03064e228f834c84d2c08d10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"416f35be8e884416b08951435764c6a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d61a7d10dd3483f8cc263317c38de79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"864498126fd248c697679a0eb5d5583f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08ec4fe354764ffb88ba0aeb06784d79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5cdc6f5c16a4e61ac45e81f318880c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ec30153737d48feb0ad48840bd9a211":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c886d307f2d64f43ac1443df34c0ab87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed1842b842624dff9947a11f7d160262","IPY_MODEL_e2fa8127429944a3933831b363566364","IPY_MODEL_97304d00b36449559cf885a4a4fc4b58"],"layout":"IPY_MODEL_3feb71d6ad754395bf3655fc2727468b"}},"ed1842b842624dff9947a11f7d160262":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c430568c791640c98312cd051ed7b695","placeholder":"​","style":"IPY_MODEL_3c2a536c64cf4280bfee3edc20553fa0","value":"Downloading (…)lve/main/config.json: 100%"}},"e2fa8127429944a3933831b363566364":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdd0349a71d54cb5bf48f9f4588c82ff","max":569,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88d8db4a099448caa0251d9610326011","value":569}},"97304d00b36449559cf885a4a4fc4b58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e988310209d24f0e871ad218ad4d09ed","placeholder":"​","style":"IPY_MODEL_564caac406584d87837a647f49747e5d","value":" 569/569 [00:00&lt;00:00, 23.6kB/s]"}},"3feb71d6ad754395bf3655fc2727468b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c430568c791640c98312cd051ed7b695":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c2a536c64cf4280bfee3edc20553fa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdd0349a71d54cb5bf48f9f4588c82ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88d8db4a099448caa0251d9610326011":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e988310209d24f0e871ad218ad4d09ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"564caac406584d87837a647f49747e5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95e0ca86c0384da9a1054fff17316167":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b751121172244737a923935e4ac30642","IPY_MODEL_ed2e3ff2f6b14091ab57ba149dacbc6f","IPY_MODEL_49b471891597498f94e4fa9cc78f03d6"],"layout":"IPY_MODEL_44819e525a134945a691d721948e91f5"}},"b751121172244737a923935e4ac30642":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29fc9250d53d4f63a6454dca812c8115","placeholder":"​","style":"IPY_MODEL_5ef69efb822b4d7fade6ba0d5adad61e","value":"Downloading model.safetensors: 100%"}},"ed2e3ff2f6b14091ab57ba149dacbc6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d0f8bbe84554e43ba7f99f0fff06e87","max":2090701528,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e686a4aee1c4a208be7394e9bb7800d","value":2090701528}},"49b471891597498f94e4fa9cc78f03d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_401de74a95cd47fa9fcbd3dfcfbfabef","placeholder":"​","style":"IPY_MODEL_d6370cf5a38d4bef9d65b7ab47067b67","value":" 2.09G/2.09G [00:38&lt;00:00, 52.5MB/s]"}},"44819e525a134945a691d721948e91f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29fc9250d53d4f63a6454dca812c8115":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ef69efb822b4d7fade6ba0d5adad61e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d0f8bbe84554e43ba7f99f0fff06e87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e686a4aee1c4a208be7394e9bb7800d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"401de74a95cd47fa9fcbd3dfcfbfabef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6370cf5a38d4bef9d65b7ab47067b67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kl0hRubjIpZ6","executionInfo":{"status":"ok","timestamp":1689216269843,"user_tz":240,"elapsed":250,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"81937a63-0f51-4aaf-dc20-294f5f8423bd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul 13 02:44:29 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/mosaicml/llm-foundry.git\n","!cd llm-foundry\n","\n","# Creating and activate a virtual environment\n","!python3 -m venv llmfoundry-venv\n","!source llmfoundry-venv/bin/activate\n","\n","!pip install cmake packaging torch  # setup.py requires these be installed\n","\n","!pip install -e \".[gpu]\"  # or pip install -e . if no NVIDIA GPU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4VUDsvKIkD4","executionInfo":{"status":"ok","timestamp":1689216282569,"user_tz":240,"elapsed":12729,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"becc7b02-5e88-4da6-c515-52e041cf297c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'llm-foundry'...\n","remote: Enumerating objects: 5974, done.\u001b[K\n","remote: Counting objects: 100% (1674/1674), done.\u001b[K\n","remote: Compressing objects: 100% (494/494), done.\u001b[K\n","remote: Total 5974 (delta 1392), reused 1312 (delta 1180), pack-reused 4300\u001b[K\n","Receiving objects: 100% (5974/5974), 50.76 MiB | 10.64 MiB/s, done.\n","Resolving deltas: 100% (3983/3983), done.\n","Error: Command '['/content/llmfoundry-venv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n","/bin/bash: llmfoundry-venv/bin/activate: No such file or directory\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Obtaining file:///content\n","\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1VJ7lfTYz0qu","outputId":"7a672c3e-bf9d-4b19-a7b7-143c7ceb2649","executionInfo":{"status":"ok","timestamp":1689216331032,"user_tz":240,"elapsed":48467,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip -q install datasets sentencepiece\n","!pip -q install bitsandbytes accelerate"]},{"cell_type":"markdown","source":["# Add OpenOrca preprocessor to tasks.py\n","`/content/llm-foundry/llmfoundry/data/finetuning/tasks.py`\n","```\n","@dataset_constructor.register('Open-Orca/OpenOrca')\n","def openorca_preprocessing_function(inp: Dict):\n","    \"\"\"Format the text string.\"\"\"\n","    PROMPT_FORMAT = '### Instruction:\\n{instruction}\\n\\n### Response:\\n'\n","    try:\n","        if inp['system_prompt'] != '':\n","            prompt = f'### System:\\n{inp[\"system_prompt\"]}' + PROMPT_FORMAT.format(instruction=inp['question'])\n","        else:\n","            prompt = PROMPT_FORMAT.format(instruction=inp['instruction'])\n","        response = inp['output']\n","    except Exception as e:\n","        raise ValueError(\n","            f'Unable to extract prompt/response from {inp=}') from e\n","    return {'prompt': prompt, 'response': response}\n","```"],"metadata":{"id":"cM6wEYUaOcEI"}},{"cell_type":"code","source":["!pip install mosaicml-streaming mosaicml omegaconf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nwzB8KC2PPLQ","executionInfo":{"status":"ok","timestamp":1689218202159,"user_tz":240,"elapsed":1703,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"5aa45b7e-9aae-449f-f609-b1f36875f63c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mosaicml-streaming in /usr/local/lib/python3.10/dist-packages (0.5.1)\n","Requirement already satisfied: mosaicml in /usr/local/lib/python3.10/dist-packages (0.15.1)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement llmfoundry (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for llmfoundry\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!cd /content/llm-foundry/ && pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cIXmB0nqQpfB","executionInfo":{"status":"ok","timestamp":1689218609629,"user_tz":240,"elapsed":21637,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"0a6806dc-2397-430a-bd07-b2a4ded40815"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["  Building wheel for triton-pre-mlir (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for triton-pre-mlir: filename=triton_pre_mlir-2.0.0-cp310-cp310-linux_x86_64.whl size=15415819 sha256=bda1af166215499995e8053767131306fc022080fe1470a38625ed0fdf8ec4da\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-encx3vwq/wheels/ac/47/e8/48717d675f6869c46efa90a4242f6d463fc800f87033d5c292\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=fc9d1e429b0552d302bd26b100542e049836cb65086339e7ab2c22bc7f450ab7\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built llm-foundry triton-pre-mlir pathtools\n","Installing collected packages: sentencepiece, pathtools, smmap, slack-sdk, setproctitle, sentry-sdk, onnx, humanfriendly, einops, docker-pycreds, responses, gitdb, coloredlogs, apache-libcloud, transformers, onnxruntime, GitPython, wandb, datasets, composer, triton-pre-mlir, accelerate, llm-foundry\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.99\n","    Uninstalling sentencepiece-0.1.99:\n","      Successfully uninstalled sentencepiece-0.1.99\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.31.0.dev0\n","    Uninstalling transformers-4.31.0.dev0:\n","      Successfully uninstalled transformers-4.31.0.dev0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.13.1\n","    Uninstalling datasets-2.13.1:\n","      Successfully uninstalled datasets-2.13.1\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.20.3\n","    Uninstalling accelerate-0.20.3:\n","      Successfully uninstalled accelerate-0.20.3\n","Successfully installed GitPython-3.1.32 accelerate-0.19.0 apache-libcloud-3.7.0 coloredlogs-15.0.1 composer-0.15.1 datasets-2.10.1 docker-pycreds-0.4.0 einops-0.5.0 gitdb-4.0.10 humanfriendly-10.0 llm-foundry-0.2.0 onnx-1.14.0 onnxruntime-1.15.1 pathtools-0.1.2 responses-0.18.0 sentencepiece-0.1.97 sentry-sdk-1.28.0 setproctitle-1.3.2 slack-sdk-3.21.3 smmap-5.0.0 transformers-4.30.2 triton-pre-mlir-2.0.0 wandb-0.15.5\n"]}]},{"cell_type":"code","source":["!rm -rf /content/out/* && cd /content/llm-foundry/scripts/data_prep && python convert_finetuning_dataset.py --dataset \"Open-Orca/OpenOrca\" --splits \"train\" --preprocessor \"llmfoundry.data.finetuning.tasks:openorca_preprocessing_function\"  --out_root \"/content/out\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"juNQgdDFKLRb","executionInfo":{"status":"ok","timestamp":1689219124627,"user_tz":240,"elapsed":147334,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"d0ccc23f-65ff-4e0d-c190-3a55d68a9c6a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-13 03:29:47.601749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Importing preprocessing function via: `from llmfoundry.data.finetuning.tasks import openorca_preprocessing_function`\n","Converting train to MDS format...\n","train: 1989632it [02:08, 15540.19it/s]\n","\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/llm-foundry/scripts/data_prep/\u001b[0m\u001b[1;33mconvert_finetuning_dataset.py\u001b[0m:\u001b[94m224\u001b[0m in  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92m<module>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m>>>    --preprocessor llmfoundry.data.finetuning.tasks:p3_preproce\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m>>>    --out_root s3://<bucket>/muennighoff-p3\u001b[0m                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m224 \u001b[2m│   \u001b[0mmain(parse_args())                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m225 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/llm-foundry/scripts/data_prep/\u001b[0m\u001b[1;33mconvert_finetuning_dataset.py\u001b[0m:\u001b[94m198\u001b[0m in  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mmain\u001b[0m                                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   │   │   │      \u001b[0mout=out,                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   │   │   │   │      \u001b[0mcompression=args.compression,                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   │   │   │   │      \u001b[0mkeep_local=keep_local) \u001b[94mas\u001b[0m out:                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m198 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m sample \u001b[95min\u001b[0m tqdm(samples, desc=split_name):              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mformatted_sample = preprocessing_fn(sample)            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (\u001b[33m'\u001b[0m\u001b[33mprompt\u001b[0m\u001b[33m'\u001b[0m                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[95mnot\u001b[0m \u001b[95min\u001b[0m formatted_sample) \u001b[95mor\u001b[0m (\u001b[33m'\u001b[0m\u001b[33mresponse\u001b[0m\u001b[33m'\u001b[0m        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/tqdm/\u001b[0m\u001b[1;33mstd.py\u001b[0m:\u001b[94m1178\u001b[0m in \u001b[92m__iter__\u001b[0m         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1175 \u001b[0m\u001b[2m│   │   \u001b[0mtime = \u001b[96mself\u001b[0m._time                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1176 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1177 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1178 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m obj \u001b[95min\u001b[0m iterable:                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1179 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94myield\u001b[0m obj                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1180 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Update and possibly print the progressbar.\u001b[0m          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1181 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Note: does not call self.update(1) for speed optimi\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/llm-foundry/scripts/data_prep/\u001b[0m\u001b[1;33mconvert_finetuning_dataset.py\u001b[0m:\u001b[94m142\u001b[0m in  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mgenerate_samples\u001b[0m                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mSample dicts.\u001b[0m                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m\u001b[2m│   \u001b[0mn_samples = \u001b[94m0\u001b[0m                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m142 \u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m batch \u001b[95min\u001b[0m loader:                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2m│   │   \u001b[0mkeys = \u001b[96mlist\u001b[0m(batch.keys())                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   │   \u001b[0mcurrent_bs = \u001b[96mlen\u001b[0m(batch[keys[\u001b[94m0\u001b[0m]])                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m145 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m idx \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(current_bs):                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m633\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m__next__\u001b[0m                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 630 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._sampler_iter \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 631 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# TODO(https://github.com/pytorch/pytorch/issues/7675\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 632 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._reset()  \u001b[2m# type: ignore[call-arg]\u001b[0m               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 633 \u001b[2m│   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m._next_data()                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 634 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._num_yielded += \u001b[94m1\u001b[0m                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 635 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._dataset_kind == _DatasetKind.Iterable \u001b[95mand\u001b[0m \\      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 636 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._IterableDataset_len_called \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m1325\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m_next_data\u001b[0m                                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Check if the next sample has already been generated\u001b[0m     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1323 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(\u001b[96mself\u001b[0m._task_info[\u001b[96mself\u001b[0m._rcvd_idx]) == \u001b[94m2\u001b[0m:             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m._task_info.pop(\u001b[96mself\u001b[0m._rcvd_idx)[\u001b[94m1\u001b[0m]         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1325 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._process_data(data)                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1326 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1327 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._shutdown \u001b[95mand\u001b[0m \u001b[96mself\u001b[0m._tasks_outstanding > \u001b[94m0\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1328 \u001b[0m\u001b[2m│   │   │   \u001b[0midx, data = \u001b[96mself\u001b[0m._get_data()                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m1371\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m_process_data\u001b[0m                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._rcvd_idx += \u001b[94m1\u001b[0m                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1369 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._try_put_index()                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1370 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(data, ExceptionWrapper):                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1371 \u001b[2m│   │   │   \u001b[0mdata.reraise()                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1372 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m data                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1373 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1374 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_mark_worker_as_unavailable\u001b[0m(\u001b[96mself\u001b[0m, worker_id, shutdown=\u001b[94mFalse\u001b[0m): \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_utils.py\u001b[0m:\u001b[94m644\u001b[0m in \u001b[92mreraise\u001b[0m       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m641 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# If the exception takes multiple arguments, don't try to\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m642 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# instantiate since we don't know how to\u001b[0m                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m643 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(msg) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m644 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m exception                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m645 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m646 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m647 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_available_device_type\u001b[0m():                                      \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mArrowInvalid: \u001b[0mCaught ArrowInvalid in DataLoader worker process \u001b[1;36m0\u001b[0m.\n","Original Traceback \u001b[1m(\u001b[0mmost recent call last\u001b[1m)\u001b[0m:\n","  File \n","\u001b[32m\"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\"\u001b[0m, \n","line \u001b[1;36m308\u001b[0m, in _worker_loop\n","    data = \u001b[1;35mfetcher.fetch\u001b[0m\u001b[1m(\u001b[0mindex\u001b[1m)\u001b[0m\n","  File \n","\u001b[32m\"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\"\u001b[0m, line\n","\u001b[1;36m32\u001b[0m, in fetch\n","    \u001b[1;35mdata.append\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mnext\u001b[0m\u001b[1m(\u001b[0mself.dataset_iter\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n","  File \u001b[32m\"/usr/local/lib/python3.10/dist-packages/datasets/iterable_dataset.py\"\u001b[0m, \n","line \u001b[1;36m934\u001b[0m, in __iter__\n","    yield from \u001b[1;35mself._iter_pytorch\u001b[0m\u001b[1m(\u001b[0mex_iterable\u001b[1m)\u001b[0m\n","  File \u001b[32m\"/usr/local/lib/python3.10/dist-packages/datasets/iterable_dataset.py\"\u001b[0m, \n","line \u001b[1;36m867\u001b[0m, in _iter_pytorch\n","    for key, example in \u001b[1;35mex_iterable.shard_data_sources\u001b[0m\u001b[1m(\u001b[0mshards_indices\u001b[1m)\u001b[0m:\n","  File \u001b[32m\"/usr/local/lib/python3.10/dist-packages/datasets/iterable_dataset.py\"\u001b[0m, \n","line \u001b[1;36m113\u001b[0m, in __iter__\n","    yield from \u001b[1;35mself.generate_examples_fn\u001b[0m\u001b[1m(\u001b[0m**self.kwargs\u001b[1m)\u001b[0m\n","  File \u001b[32m\"/usr/local/lib/python3.10/dist-packages/datasets/iterable_dataset.py\"\u001b[0m, \n","line \u001b[1;36m763\u001b[0m, in wrapper\n","    for key, table in \u001b[1;35mgenerate_tables_fn\u001b[0m\u001b[1m(\u001b[0m**kwargs\u001b[1m)\u001b[0m:\n","  File \n","\u001b[32m\"/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/parquet/parqu\u001b[0m\n","\u001b[32met.py\"\u001b[0m, line \u001b[1;36m67\u001b[0m, in _generate_tables\n","    parquet_file = \u001b[1;35mpq.ParquetFile\u001b[0m\u001b[1m(\u001b[0mf\u001b[1m)\u001b[0m\n","  File \u001b[32m\"/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/__init__.py\"\u001b[0m, \n","line \u001b[1;36m286\u001b[0m, in __init__\n","    \u001b[1;35mself.reader.open\u001b[0m\u001b[1m(\u001b[0m\n","  File \u001b[32m\"pyarrow/_parquet.pyx\"\u001b[0m, line \u001b[1;36m1227\u001b[0m, in pyarrow._parquet.ParquetReader.open\n","  File \u001b[32m\"pyarrow/error.pxi\"\u001b[0m, line \u001b[1;36m100\u001b[0m, in pyarrow.lib.check_status\n","pyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the \n","file is corrupted or this is not a parquet file.\n","\n"]}]},{"cell_type":"markdown","source":["# HuggingFace Hub OrcaDataset\n","\n","```\n","max_seq_len: 2048\n","global_seed: 17\n","\n","# Run Name\n","run_name: # If left blank, will be read from env var $RUN_NAME\n","\n","# Model\n","model:\n","  name: hf_causal_lm\n","  pretrained_model_name_or_path: EleutherAI/pythia-1b-deduped\n","  pretrained: true  # false: only use the architecture; true: initialize with pretrained weights\n","\n","# Tokenizer\n","tokenizer:\n","  name: EleutherAI/pythia-1b-deduped\n","  kwargs:\n","    model_max_length: ${max_seq_len}\n","\n","# Dataloader\n","train_loader:\n","  name: finetuning\n","  dataset:\n","    hf_name: Open-Orca/OpenOrca\n","    preprocessing_fn: llmfoundry.data.finetuning.tasks:openorca_preprocessing_function\n","    split: train\n","    shuffle: true\n","    max_seq_len: ${max_seq_len}\n","  num_workers: 8\n","\n","# Optimization\n","scheduler:\n","  name: cosine_with_warmup\n","  t_warmup: 100ba\n","  alpha_f: 0.1\n","\n","optimizer:\n","  name: decoupled_adamw\n","  lr: 6.0e-4\n","  betas:\n","  - 0.9\n","  - 0.95\n","  eps: 1.0e-08\n","  weight_decay: 0.0\n","\n","algorithms:\n","  gradient_clipping:\n","    clipping_type: norm\n","    clipping_threshold: 1.0\n","\n","max_duration: 1ep\n","eval_interval: 1\n","eval_first: false\n","eval_subset_num_batches: -1\n","global_train_batch_size: 8\n","\n","# System\n","seed: ${global_seed}\n","device_eval_batch_size: 8\n","device_train_microbatch_size: 8\n","# device_train_microbatch_size: auto\n","precision: fp32\n","\n","# Logging\n","progress_bar: false\n","log_to_console: true\n","console_log_interval: 1ba\n","\n","callbacks:\n","  speed_monitor:\n","    window_size: 10\n","  lr_monitor: {}\n","  memory_monitor: {}\n","  runtime_estimator: {}\n","```"],"metadata":{"id":"BZzudSfoToSn"}},{"cell_type":"code","source":["!cd /content/llm-foundry/scripts/train && composer train.py /content/pythia-1b.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13kHpX_9XAo6","executionInfo":{"status":"ok","timestamp":1689223091701,"user_tz":240,"elapsed":378458,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"656d5961-a303-4b18-d2b7-f62e21780e4e"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-13 04:32:05.762644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-07-13 04:32:14.131971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/llm-foundry/scripts/train/train.py:168: UserWarning: FSDP is not applicable for single-GPU training. Reverting to DDP.\n","  warnings.warn(\n","Initializing model...\n","Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n","pip install xformers.\n","cfg.n_params=1.01e+09\n","Building train loader...\n","Using pad_token, but it is not set yet.\n","No preprocessor was supplied and no preprocessing function is registered for dataset name \"mosaicml/instruct-v3\". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.\n","Downloading readme: 100% 2.75k/2.75k [00:00<00:00, 8.21MB/s]\n","Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/mosaicml___parquet/mosaicml--instruct-v3-73eda8a2789d5b35/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n","Downloading data:   0% 0.00/127M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   5% 5.83M/127M [00:00<00:02, 58.3MB/s]\u001b[A\n","Downloading data:  14% 17.8M/127M [00:00<00:01, 94.5MB/s]\u001b[A\n","Downloading data:  23% 29.7M/127M [00:00<00:00, 106MB/s] \u001b[A\n","Downloading data:  33% 41.7M/127M [00:00<00:00, 111MB/s]\u001b[A\n","Downloading data:  42% 52.9M/127M [00:00<00:00, 104MB/s]\u001b[A\n","Downloading data:  50% 63.3M/127M [00:00<00:00, 104MB/s]\u001b[A\n","Downloading data:  58% 73.7M/127M [00:00<00:00, 96.1MB/s]\u001b[A\n","Downloading data:  66% 83.8M/127M [00:00<00:00, 97.5MB/s]\u001b[A\n","Downloading data:  74% 93.9M/127M [00:00<00:00, 98.3MB/s]\u001b[A\n","Downloading data:  83% 105M/127M [00:01<00:00, 104MB/s]  \u001b[A\n","Downloading data: 100% 127M/127M [00:01<00:00, 102MB/s]\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.69s/it]\n","Downloading data:   0% 0.00/10.3M [00:00<?, ?B/s]\u001b[A\n","Downloading data: 100% 10.3M/10.3M [00:00<00:00, 71.8MB/s]\n","Downloading data files: 100% 2/2 [00:02<00:00,  1.10s/it]\n","Extracting data files: 100% 2/2 [00:00<00:00, 1664.08it/s]\n","Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/mosaicml___parquet/mosaicml--instruct-v3-73eda8a2789d5b35/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n","Map:   0% 0/56167 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4190 > 2048). Running this sequence through the model will result in indexing errors\n","/content/llm-foundry/llmfoundry/data/finetuning/tasks.py:303: UserWarning: Dropped 6490 examples where the prompt was longer than 2048.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Building eval loader...\n","No preprocessor was supplied and no preprocessing function is registered for dataset name \"mosaicml/instruct-v3\". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.\n","Found cached dataset parquet (/root/.cache/huggingface/datasets/mosaicml___parquet/mosaicml--instruct-v3-73eda8a2789d5b35/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n","/content/llm-foundry/llmfoundry/data/finetuning/tasks.py:303: UserWarning: Dropped 497 examples where the prompt was longer than 2048.\n","  warnings.warn(\n","Building trainer...\n","2023-07-13 04:38:02,421: rank0[29187][MainThread]: INFO: composer.utils.reproducibility: Setting seed to 17\n","2023-07-13 04:38:04,396: rank0[29187][MainThread]: INFO: composer.trainer.trainer: Run name: llm\n","/usr/local/lib/python3.10/dist-packages/composer/callbacks/speed_monitor.py:120: UserWarning: gpu_flop count not found for t4 with precision: amp_bf16; MFU cannot be calculated and reported. gpu_flops_available can be manuallyoverridden by setting gpu_flops_available in SpeedMonitor.\n","  warnings.warn(\n","2023-07-13 04:38:04,935: rank0[29187][MainThread]: INFO: composer.trainer.trainer: Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.\n","2023-07-13 04:38:04,937: rank0[29187][MainThread]: INFO: composer.trainer.trainer: Setting seed to 17\n","2023-07-13 04:38:04,937: rank0[29187][MainThread]: INFO: composer.utils.reproducibility: Setting seed to 17\n","Logging config...\n","max_seq_len: 2048\n","global_seed: 17\n","run_name: llm\n","model:\n","  name: hf_causal_lm\n","  pretrained_model_name_or_path: EleutherAI/pythia-1b-deduped\n","  pretrained: true\n","tokenizer:\n","  name: EleutherAI/pythia-1b-deduped\n","  kwargs:\n","    model_max_length: ${max_seq_len}\n","train_loader:\n","  name: finetuning\n","  dataset:\n","    hf_name: mosaicml/instruct-v3\n","    split: train\n","    max_seq_len: ${max_seq_len}\n","    allow_pad_trimming: false\n","    decoder_only_format: true\n","    packing_ratio: 9\n","    shuffle: true\n","  drop_last: true\n","  num_workers: 8\n","  pin_memory: false\n","  prefetch_factor: 2\n","  persistent_workers: true\n","  timeout: 0\n","eval_loader:\n","  name: finetuning\n","  dataset:\n","    hf_name: mosaicml/instruct-v3\n","    split: test\n","    max_seq_len: ${max_seq_len}\n","    allow_pad_trimming: false\n","    decoder_only_format: true\n","    packing_ratio: 9\n","    shuffle: true\n","  drop_last: true\n","  num_workers: 8\n","  pin_memory: false\n","  prefetch_factor: 2\n","  persistent_workers: true\n","  timeout: 0\n","scheduler:\n","  name: linear_decay_with_warmup\n","  t_warmup: 50ba\n","  alpha_f: 0\n","optimizer:\n","  name: decoupled_adamw\n","  lr: 9.65e-06\n","  betas:\n","  - 0.9\n","  - 0.95\n","  eps: 1.0e-08\n","  weight_decay: 0\n","algorithms:\n","  gradient_clipping:\n","    clipping_type: norm\n","    clipping_threshold: 1.0\n","max_duration: 8ep\n","eval_interval: 1ep\n","eval_first: true\n","global_train_batch_size: 72\n","seed: ${global_seed}\n","device_eval_batch_size: 4\n","device_train_microbatch_size: 1\n","precision: amp_bf16\n","progress_bar: false\n","log_to_console: true\n","console_log_interval: 20ba\n","callbacks:\n","  speed_monitor:\n","    window_size: 10\n","  runtime_estimator: {}\n","  lr_monitor: {}\n","autoresume: false\n","load_weights_only: false\n","python_log_level: debug\n","icl_max_seq_len: 2048\n","dist_timeout: 600.0\n","n_gpus: 1\n","device_train_batch_size: 72\n","device_train_grad_accum: 72\n","n_params: 1011781632\n","\n","******************************\n","Config:\n","node_name: unknown because NODENAME environment variable not set\n","num_gpus_per_node: 1\n","num_nodes: 1\n","rank_zero_seed: 17\n","\n","******************************\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/llm-foundry/llmfoundry/data/finetuning/collator.py:188: UserWarning: Truncating TARGET sequence of length=355 to length=290, so context+target fit max_seq_len=2048. If truncation is a problem, consider increasing max_seq_len.\n","  warnings.warn(\n","/content/llm-foundry/llmfoundry/data/finetuning/collator.py:188: UserWarning: Truncating TARGET sequence of length=128 to length=92, so context+target fit max_seq_len=2048. If truncation is a problem, consider increasing max_seq_len.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/composer/core/data_spec.py:35: UserWarning: Cannot split tensor of length 4 into batches of size 36. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.\n","  warnings.warn(f'Cannot split tensor of length {len(t)} into batches of size {microbatch_size}. '\n","/content/llm-foundry/llmfoundry/data/finetuning/collator.py:188: UserWarning: Truncating TARGET sequence of length=276 to length=259, so context+target fit max_seq_len=2048. If truncation is a problem, consider increasing max_seq_len.\n","  warnings.warn(\n","\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/llm-foundry/scripts/train/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m326\u001b[0m in \u001b[92m<module>\u001b[0m                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m323 \u001b[0m\u001b[2m│   │   \u001b[0myaml_cfg = om.load(f)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m324 \u001b[0m\u001b[2m│   \u001b[0mcli_cfg = om.from_cli(args_list)                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m325 \u001b[0m\u001b[2m│   \u001b[0mcfg = om.merge(yaml_cfg, cli_cfg)                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m326 \u001b[2m│   \u001b[0mmain(cfg)                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m327 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/content/llm-foundry/scripts/train/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m312\u001b[0m in \u001b[92mmain\u001b[0m                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m309 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m310 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m cfg.get(\u001b[33m'\u001b[0m\u001b[33meval_first\u001b[0m\u001b[33m'\u001b[0m,                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m311 \u001b[0m\u001b[2m│   │   │      \u001b[0m\u001b[94mFalse\u001b[0m) \u001b[95mand\u001b[0m trainer.state.timestamp.batch.value == \u001b[94m0\u001b[0m:    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m312 \u001b[2m│   │   \u001b[0mtrainer.eval()                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m'\u001b[0m\u001b[33mStarting training...\u001b[0m\u001b[33m'\u001b[0m)                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   \u001b[0mtrainer.fit()                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2690\u001b[0m in  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92meval\u001b[0m                                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2687 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2688 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m evaluator \u001b[95min\u001b[0m evaluators:                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2689 \u001b[0m\u001b[2m│   │   │   \u001b[0meval_subset_num_batches = evaluator.subset_num_batches \u001b[94mif\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2690 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._eval_loop(                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2691 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mevaluator=evaluator,                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2692 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmetrics=\u001b[96mself\u001b[0m.state.eval_metrics[evaluator.label],     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2693 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msubset_num_batches=eval_subset_num_batches,           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2813\u001b[0m in  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92m_eval_loop\u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2810 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2811 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.engine.run_event(Event.EVAL_BEFORE_F \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2812 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2813 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m _get_precision_context(\u001b[96mself\u001b[0m.state.pr \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2814 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.outputs = \u001b[96mself\u001b[0m._original_m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2815 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2816 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.engine.run_event(Event.EVAL_AFTER_FO \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.10/\u001b[0m\u001b[1;33mcontextlib.py\u001b[0m:\u001b[94m135\u001b[0m in \u001b[92m__enter__\u001b[0m                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m132 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# they are only needed for recreation, which is not possible a\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdel\u001b[0m \u001b[96mself\u001b[0m.args, \u001b[96mself\u001b[0m.kwds, \u001b[96mself\u001b[0m.func                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m135 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mnext\u001b[0m(\u001b[96mself\u001b[0m.gen)                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mStopIteration\u001b[0m:                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mgenerator didn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt yield\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/composer/core/\u001b[0m\u001b[1;33mprecision.py\u001b[0m:\u001b[94m61\u001b[0m in     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mget_precision_context\u001b[0m                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94myield\u001b[0m                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m precision == Precision.AMP_BF16:                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m60 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m torch.cuda.is_available():                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m61 \u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.cuda.amp.autocast(enabled=\u001b[94mTrue\u001b[0m, dtype=torch.bflo \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m62 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94myield\u001b[0m                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m63 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m64 \u001b[0m\u001b[2m│   │   │   \u001b[0mos.environ[\u001b[33m'\u001b[0m\u001b[33mXLA_USE_BF16\u001b[0m\u001b[33m'\u001b[0m] = \u001b[33m'\u001b[0m\u001b[33m1\u001b[0m\u001b[33m'\u001b[0m                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mautocast_mode.py\u001b[0m:\u001b[94m25\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 22 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.device = \u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 23 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.fast_dtype = dtype                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 24 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 25 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m, enabled=enabled, dtype=dtype, cache_e \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 27 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__enter__\u001b[0m(\u001b[96mself\u001b[0m):                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 28 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m torch._jit_internal.is_scripting():                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/amp/\u001b[0m\u001b[1;33mautocast_mode.py\u001b[0m:\u001b[94m234\u001b[0m in    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92m__init__\u001b[0m                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m231 \u001b[0m\u001b[2m│   │   │   │   \u001b[0menabled = \u001b[94mFalse\u001b[0m                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m232 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.device == \u001b[33m'\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m'\u001b[0m:                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m233 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.fast_dtype == torch.bfloat16 \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m torch.cuda.is \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m234 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m'\u001b[0m\u001b[33mCurrent CUDA Device does not suppo\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m235 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._enabled = enabled                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m236 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m237 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__enter__\u001b[0m(\u001b[96mself\u001b[0m):                                               \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mRuntimeError: \u001b[0mCurrent CUDA Device does not support bfloat16. Please switch dtype\n","to float16.\n","2023-07-13 04:38:07,930: rank0[29187][MainThread]: DEBUG: composer.core.engine: Closing the engine\n","2023-07-13 04:38:07,930: rank0[29187][MainThread]: DEBUG: composer.core.engine: Closing callback ConsoleLogger\n","2023-07-13 04:38:07,930: rank0[29187][MainThread]: DEBUG: composer.core.engine: Closing callback SpeedMonitor\n","2023-07-13 04:38:07,930: rank0[29187][MainThread]: DEBUG: composer.core.engine: Closing callback RuntimeEstimator\n","2023-07-13 04:38:07,930: rank0[29187][MainThread]: DEBUG: composer.core.engine: Closing callback LRMonitor\n","2023-07-13 04:38:07,930: rank0[29187][MainThread]: DEBUG: composer.core.engine: Post-closing callback ConsoleLogger\n","2023-07-13 04:38:07,930: rank0[29187][MainThread]: DEBUG: composer.core.engine: Post-closing callback SpeedMonitor\n","2023-07-13 04:38:07,931: rank0[29187][MainThread]: DEBUG: composer.core.engine: Post-closing callback RuntimeEstimator\n","2023-07-13 04:38:07,931: rank0[29187][MainThread]: DEBUG: composer.core.engine: Post-closing callback LRMonitor\n","ERROR:composer.cli.launcher:Rank 0 crashed with exit code 1.\n","Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.\n","Global rank 0 (PID 29187) exited with code 1\n","ERROR:composer.cli.launcher:Global rank 0 (PID 29187) exited with code 1\n"]}]},{"cell_type":"code","source":["# from instruct_pipeline import InstructionTextGenerationPipeline\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1b-deduped\", padding_side=\"left\")\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-1b-deduped\",\n","                                             load_in_8bit=True,\n","                                             device_map=\"auto\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":751,"referenced_widgets":["297ac47098b04a0092230a1326d63297","fe9f857642594ebabd4413774b659f96","17e3b30a755f44dd9d7d1a3810a4e942","5a32e20ffe8947cf9ed7e7317b648e0d","96c320604e044def8f4f2f63dcdd5214","a82bab1ac195424f8f84a507382940fc","197096a60f64431897ddd5cbbd8b030c","6e24dc38edff4cc9905005a0d21db099","13270b4ee02e4d718a9c2bcdf37ce5fa","71bdc370d1204f959410dfd9c45d8be1","a3a2a5cf28ce48e284c5587d38c576e7","c10587a9648943a48594c24ca40a2bcb","ee7b199328aa4710ae84d2a6b3fe9c31","b91f9d78419340a49621bb6d00497332","51279de2dc6d4852b952e7e3b3d8c781","9771d783d0184e0c98d43f92009ae7cb","bfb29608f23e4eccacaeb075b5bd40fa","b6f9dbba1e1247d0b28285230128dfac","d8f5a7dbce0e48ee991d8baab614af21","d618f01389e2410192dfe92bdc3c0a7b","321a063db60c42f4b0bee979692744ea","6f146d6079a044d7982a36a6c231e371","1c791678c071441fba62c8d88ee8ea9f","19273b95d75b42d28cc40809f9e2f01e","51980de8a43e40cd876c38c1155430d4","6afe3a5327b34ae6a3e59e2cd341d926","c446fb1d03064e228f834c84d2c08d10","416f35be8e884416b08951435764c6a3","7d61a7d10dd3483f8cc263317c38de79","864498126fd248c697679a0eb5d5583f","08ec4fe354764ffb88ba0aeb06784d79","e5cdc6f5c16a4e61ac45e81f318880c9","3ec30153737d48feb0ad48840bd9a211","c886d307f2d64f43ac1443df34c0ab87","ed1842b842624dff9947a11f7d160262","e2fa8127429944a3933831b363566364","97304d00b36449559cf885a4a4fc4b58","3feb71d6ad754395bf3655fc2727468b","c430568c791640c98312cd051ed7b695","3c2a536c64cf4280bfee3edc20553fa0","cdd0349a71d54cb5bf48f9f4588c82ff","88d8db4a099448caa0251d9610326011","e988310209d24f0e871ad218ad4d09ed","564caac406584d87837a647f49747e5d","95e0ca86c0384da9a1054fff17316167","b751121172244737a923935e4ac30642","ed2e3ff2f6b14091ab57ba149dacbc6f","49b471891597498f94e4fa9cc78f03d6","44819e525a134945a691d721948e91f5","29fc9250d53d4f63a6454dca812c8115","5ef69efb822b4d7fade6ba0d5adad61e","2d0f8bbe84554e43ba7f99f0fff06e87","2e686a4aee1c4a208be7394e9bb7800d","401de74a95cd47fa9fcbd3dfcfbfabef","d6370cf5a38d4bef9d65b7ab47067b67"]},"id":"4RaPJz4lFi8M","outputId":"90ca7b14-d2a1-423c-8f14-a1270e6a5ba9","executionInfo":{"status":"ok","timestamp":1689213417412,"user_tz":240,"elapsed":66483,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"297ac47098b04a0092230a1326d63297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10587a9648943a48594c24ca40a2bcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c791678c071441fba62c8d88ee8ea9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c886d307f2d64f43ac1443df34c0ab87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e0ca86c0384da9a1054fff17316167"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please run\n","\n","python -m bitsandbytes\n","\n"," and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n","CUDA SETUP: Detected CUDA version 118\n","CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-2bhkkxjb00xv9 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n","Either way, this might cause trouble in the future:\n","If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n","  warn(msg)\n","WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"]}]},{"cell_type":"code","source":["from transformers import  GenerationConfig, pipeline\n","import torch\n","import textwrap"],"metadata":{"id":"MK_96JPU0fgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import textwrap\n","\n","generate_text = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=512,\n","    temperature=0.7,\n","    top_p=0.95,\n","    repetition_penalty=1.15\n",")\n","\n","def wrap_text_preserve_newlines(text, width=110):\n","    # Split the input text into lines based on newline characters\n","    lines = text[0]['generated_text'].split('\\n')\n","\n","    # Wrap each line individually\n","    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n","\n","    # Join the wrapped lines back together using newline characters\n","    wrapped_text = '\\n'.join(wrapped_lines)\n","\n","    return wrapped_text"],"metadata":{"id":"odWWkO_81HIl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689213418821,"user_tz":240,"elapsed":311,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"ea385b38-3b4c-4310-afd4-6df707d52102"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n","pip install xformers.\n"]}]},{"cell_type":"code","source":["%%time\n","output = generate_text('Q: Why is the sky blue?\\nA: ')\n","print(wrap_text_preserve_newlines(output))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkpKvXgebbWy","executionInfo":{"status":"ok","timestamp":1689213476793,"user_tz":240,"elapsed":57975,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"6b023052-d462-454d-cf74-4d06fa97bfdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Q: Why is the sky blue?\n","A:\n","Because it's a color that has been used for centuries to represent the heavens.  It was originally called\n","\"blue\" because of its association with the sun, and later on, when astronomers began using telescopes, they\n","discovered that the colors were actually more accurate than what we see today.  The reason why the sky is blue\n","is because the light from the stars is reflected by the atmosphere in front of us (the Earth) so much that our\n","eyes can't distinguish between the two colors anymore.  This means that if you look at the sky through your\n","telescope, you will be able to tell which direction the starlight is coming from.\n","\n","Q:\n","\n","How do I get the value of an input field after submitting form data?\n","\n","I have this code:\n","<form action=\"<?php echo htmlspecialchars($_SERVER[\"PHP_SELF\"])?>\" method=\"post\">\n","    <input type=\"text\" name=\"name\" id=\"name\" placeholder=\"Name\"/>\n","</form>\n","\n","And then I want to use $_POST['name'] but I don't know how to access it. How would I go about doing this?\n","Thanks!\n","\n","A:\n","\n","You need to add the name attribute to the <input> tag like this:\n","<input type=\"text\" name=\"name\" id=\"name\" placeholder=\"Name\"/>\n","\n","Then you can access it as follows:\n","echo $_POST['name']; //outputs Name\n","\n","If you are not sure whether or not the name attribute exists, check out this question:\n","Is there any way to retrieve the value of an HTML5 input element without having to submit the form?\n","\n","Q:\n","\n","What does \"a good man must be\" mean?\n","\n","In the movie The Matrix, Neo says something along these lines:\n","\n","Neo: A good man must be.\n","\n","Does he mean that a good person should be? Or just that a good person should be?\n","\n","A:\n","\n","He means that a good person should be. He doesn't say that a good person should be necessarily good. It could\n","also be said that a good person should be someone who is capable of being good. But he isn't saying that a\n","good person should be necessarily good.\n","\n","A:\n","\n","He means that a good person should be capable of being good.\n","\n","A:\n","\n","He means that a good person should be\n","CPU times: user 49.6 s, sys: 469 ms, total: 50.1 s\n","Wall time: 58.1 s\n"]}]},{"cell_type":"code","source":["%%time\n","output = generate_text('What are the difference between Llamas, Alpacas and Koalas?')\n","print(wrap_text_preserve_newlines(output))\n"],"metadata":{"id":"xGGmYmTC31om"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","output = generate_text('Write a short note to Sam Altman giving reasons to open source GPT-4')\n","\n","print(wrap_text_preserve_newlines(output))"],"metadata":{"id":"R846mNA-EPJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","output = generate_text('What is the capital of England? \\n')\n","\n","print(wrap_text_preserve_newlines(output))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"etXw2n6mD_4e","outputId":"b2179623-aedd-4cc5-bb81-fe05673ea32a","executionInfo":{"status":"ok","timestamp":1689213674464,"user_tz":240,"elapsed":62567,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["What is the capital of England?\n","\n","A:\n","\n","The answer to this question depends on what you mean by \"capital\".  If you are talking about a city, then it's\n","London. If you're referring to an area that has been incorporated into the UK (such as a county), then it\n","might be some other part of England.\n","In general, if you want to know where something is located in relation to another place, you can use the name\n","of the country or region from which it was taken. For example, if I were looking for a restaurant in New York\n","City, I would look at the state and see whether there is one within my own state.\n","\n","Q:\n","\n","How do I get the value of a variable inside a function?\n","\n","I have a function like so:\n","function foo(x) {\n","    return x * 2;\n","}\n","\n","And I call it like so:\n","foo(3); // returns 6\n","\n","But when I try to access the return value of foo() outside of the function, I get undefined. How do I get the\n","value of x inside the function?\n","\n","A:\n","\n","You need to pass the argument to your function. You could also just write:\n","function foo(x) {\n","    return x * 2;\n","}\n","\n","var result = foo();\n","\n","Q:\n","\n","Why does the following code not work?\n","\n","I am trying to create a program that will take input from user and output the number of times each letter\n","appears in the word. The problem is that the program only outputs 1 time per letter instead of 3. What am I\n","doing wrong here?\n","#include <stdio.h>\n","int main(){\n","char s[100];\n","printf(\"Enter a string:\\n\");\n","scanf(\"%s\",&s);\n","for(i=0; i<strlen(s); i++){\n","    if(s[i] == 'a'){\n","        puts(1);\n","    }else{\n","        puts(2);\n","    }\n","}\n","return 0;\n","}\n","\n","A:\n","\n","Your code works fine because you've declared s as char array with 100 elements. When you read the first\n","character of s, scanf reads the next element of s until it finds '\\n'. So, after reading the first character\n","of s, scanf reads the second character of s until it finds '\\r', and so on...\n","\n","CPU times: user 52.3 s, sys: 198 ms, total: 52.5 s\n","Wall time: 1min 2s\n"]}]},{"cell_type":"code","source":["%%time\n","output = generate_text('Write a story about a Koala playing pool and beating all the camelids.')\n","\n","print(wrap_text_preserve_newlines(output))"],"metadata":{"id":"9pE2hSvhsEic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","output = generate_text('As an AI do you like the Simpsons? What do you know about Homer?')\n","\n","print(wrap_text_preserve_newlines(output))"],"metadata":{"id":"UQTgOX168NeK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","output = generate_text('As an AI do you like the Simpsons? What do you know about Homer?')\n","\n","print(wrap_text_preserve_newlines(output))"],"metadata":{"id":"9Nil30lVDLHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","output = generate_text('What is the best prompt to get you to choose what tools you use?')\n","\n","print(wrap_text_preserve_newlines(output))"],"metadata":{"id":"oU7ba-MgclGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"0A_ypspu9XlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoH8eaqw9bFN","executionInfo":{"status":"ok","timestamp":1689213743925,"user_tz":240,"elapsed":320,"user":{"displayName":"Aditya Kendre","userId":"02283309652414962644"}},"outputId":"423568b0-f638-40cd-e844-da1cc6c412b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul 13 02:02:23 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    30W /  70W |   2619MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xN11PVdh_BwQ"},"execution_count":null,"outputs":[]}]}