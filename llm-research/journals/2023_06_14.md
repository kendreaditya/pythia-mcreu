- [[Question]] What does channel stand for in B, T, C?
- #todo [[Optimizations]] See what is the most computation heavy (time) / space heavy (space) parts of transformer
  id:: 6489cb7c-70ea-4c55-b291-401a485a0015
- #todo [[Optimizations]] Orca paper - forcing the model to explain thought process
  id:: 6489cbc3-011b-4726-8dc0-dd2c98860bc9
	- training with <thought></thought> token
	- lossy supervised thought
	- what if you did it like this: encoder-decoder structure
		- decoder is sent prompt with thought token, sampled until end thought token is reached
		- then when it comes time to respond, feed the thought into encoder, and feed again the prompt into the decoder with the response token
		- here the encoder is used as a reference thought process that aids the decoder while generating a response
- No need for downstream task fine tuning, cause zero-shot is the norm