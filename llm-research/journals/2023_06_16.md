- I think the best bet is to train gp2-355m for a week for a 700ish billion, while training make a another dataset for
- Over the past week, I have been able to narrow down our options and identify key considerations for our project. Our main goal is to achieve on-device machine learning using a capable model. As you may be aware, the current trend in the field involves training large-scale models for extended periods, often reaching up to 1 trillion tokens. This approach has yielded impressive results, as exemplified by Facebook's LLaMa, a 65 billion-parameter model trained for 1.4 trillion tokens. LLaMa has achieved a level of performance comparable to chatGPT (GPT-3), which has 175 billion parameters and was trained for 410 billion tokens. Notably, LLaMa is significantly smaller, yet it maintains a comparable performance level.
- However, for our specific use case, where we aim to develop smaller models (around 500 million parameters) that can fit within the memory constraints of personal devices, the training time has not been as extensive. Models of this size, such as GPT-2 (355 million parameters), have been trained for only 10 billion tokens. Nevertheless, longer training durations are essential to ensure adequate generalization.
- Given the current scope of our project, it would be ideal to train a model from scratch using a larger dataset, targeting approximately 700 billion parameters. However, considering our time constraints, we could explore using some pretrained models. Here are a few options we can consider:
	- 1. mpt-1b: This model has 1 billion parameters and has been trained for 200 billion tokens. It shows promising results in terms of generalization but would require modifications to make it usable on local devices.
	- 2. bloom-560m: With 500 million parameters, this model has been trained for approximately 400 billion tokens. Although we haven't extensively tested it yet, it appears to be a promising candidate that would likely meet our requirements.
	- 3. GPT2-355m: This model, consisting of 355 million parameters, has been trained for 10 billion tokens. We have already confirmed that it functions, but its performance is not optimal. Additional pretraining would be necessary to improve its capabilities.
- Additionally, there are other GPT2-based models trained on larger datasets that we could potentially leverage, such as vicgalle/gpt2-alpaca-gpt4, Locutusque/gpt2-medium-conversational, and fouadbakour/gpt2-medium-shareGPT-then-Hiverse-epochs-15. These models offer increased training and parameterization compared to GPT2-355m.
- Once we finalize the model selection, the next step would involve fine-tuning the chosen model on a dataset that emphasizes instruction-based learning. This stage will require significant effort, as most of the work lies in this fine-tuning process. In this regard, we can utilize a local model as the parent/teacher, prompting it with specific queries and requesting explanations of its thought process. Recent papers, such as Orca, have emphasized the importance of incorporating logical reasoning to ensure correctness in our model's output.
- Please review the options provided and let me know your thoughts on the most suitable model for our project. If you have any additional insights or suggestions, I would greatly appreciate your input. Together, we can make an informed decision and move forward with our on-device machine learning endeavor.